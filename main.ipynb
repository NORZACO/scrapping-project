{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6238e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99372d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.tableau.com/support/known-issues\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Scraping table data\n",
    "table = soup.find(\"table\", {\"class\": \"table-list\"})\n",
    "rows = table.find_all(\"tr\")\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Issue\", \"Affected Products\", \"Status\", \"Updated\"])\n",
    "\n",
    "# Adding extraction time stamp\n",
    "df[\"Extraction Time\"] = datetime.datetime.now()\n",
    "\n",
    "#print(df)\n",
    "\n",
    "# Budget and time estimate\n",
    "#The cost of this code depends on the environment in which it will be run, such as cloud or local, and the frequency at which the data will be scraped. On a personal computer, it should take just a few seconds to run, and the cost would be minimal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cce51fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9d/x44_b9r16z18wdwzlp7y504r0000gn/T/ipykernel_29988/685895042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Scraping table data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"table\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "url = \"https://www.tableau.com/support/known-issues\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Scraping table data\n",
    "table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "rows = table.find_all(\"tr\")\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Issue\", \"Affected Products\", \"Status\", \"Updated\"])\n",
    "\n",
    "# Adding extraction time stamp\n",
    "df[\"Extraction Time\"] = datetime.datetime.now()\n",
    "\n",
    "# Printing DataFrame using tabulate\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31133e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "url = \"https://www.tableau.com/support/known-issues\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Scraping table data\n",
    "table = soup.find(\"table\", {\"class\": \"table-list\"})\n",
    "#print(table)\n",
    "rows = table.find_all(\"tr\")\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [re.sub(r'[\\n\\r\\t]', '', ele.text.strip()) for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Issue ID\", \"Product\", \"Description\", \"Status\"])\n",
    "\n",
    "# Adding extraction time stamp\n",
    "df[\"Extraction Time\"] = datetime.datetime.now()\n",
    "\n",
    "# Printing DataFrame using tabulate\n",
    "#print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c0b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "url = \"https://www.tableau.com/support/known-issues\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table with known issues\n",
    "known_issues_table = soup.find(\"table\", {\"class\": \"table-list\"})\n",
    "\n",
    "# Find all rows in the table\n",
    "table_rows = known_issues_table.find_all(\"tr\")\n",
    "\n",
    "# Create a list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through each row and scrape the data\n",
    "for row in table_rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    #print(columns)\n",
    "    columns = [element.text.strip() for element in columns]\n",
    "    data.append([element for element in columns if element])\n",
    "\n",
    "# Create a pandas DataFrame to store the scraped data\n",
    "df = pd.DataFrame(data, columns=[\"Issue ID\", \"Product\", \"Description\", \"Status\"])\n",
    "\n",
    "# Add a column to store the extraction time stamp\n",
    "df[\"Extraction Time\"] = datetime.datetime.now()\n",
    "\n",
    "# Print the DataFrame using the tabulate library\n",
    "#print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baa9521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Issue ID, Product, Description, Status]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# sending a GET request to the URL\n",
    "response = requests.get(\"https://www.tableau.com/support/known-issues?_ga=2.73464402.1191337826.1675669808-625326636.1646950374\")\n",
    "\n",
    "# check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # parse the content of the request\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # find the table in the HTML\n",
    "    table = soup.find(\"table\", attrs={\"class\": \"table-list\"})\n",
    "\n",
    "    # find the table body\n",
    "    table_body = table.find(\"tbody\")\n",
    "\n",
    "    # find all the rows in the table\n",
    "    rows = table_body.find_all(\"tr\")\n",
    "\n",
    "    # initialize an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # loop through each row in the table\n",
    "    for row in rows:\n",
    "        # find all the columns in the row\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        # extract the text from each column\n",
    "        issue_id = cols[0].text.strip()\n",
    "        #print(issue_id)\n",
    "        product = cols[1].text.strip()\n",
    "        #print(product)\n",
    "        description = cols[2].text.strip()\n",
    "        #print(description)\n",
    "        status = cols[3].text.strip()\n",
    "        #print(status)\n",
    "\n",
    "        # add the data to the list\n",
    "        data.append([issue_id, product, description, status])\n",
    "\n",
    "    # create a Pandas dataframe from the list\n",
    "    df = pd.DataFrame(data, columns=[\"Issue ID\", \"Product\", \"Description\", \"Status\"])\n",
    "\n",
    "    # add a column for the extraction time\n",
    "    #df[\"Extraction Time\"] = pd.Timestamp.now()\n",
    "\n",
    "    # display the dataframe\n",
    "    print(df[1:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3e897dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9d/x44_b9r16z18wdwzlp7y504r0000gn/T/ipykernel_31972/1752478028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Issue ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text--label table-grid--1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Product\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text--label table-grid--2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"field field--name-field-product-known-issue-text field--node field--label-hidden\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Status\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text--label table-grid--2 relative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'p'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Send a request to the website and get the HTML content\n",
    "url = \"https://www.tableau.com/support/known-issues?_ga=2.73464402.1191337826.1675669808-625326636.1646950374\"\n",
    "page = requests.get(url)\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find the table with class \"table-list\"\n",
    "table = soup.find(\"table\", class_=\"table-list\")\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "data = {\"Issue ID\": [], \"Product\": [], \"Description\": [], \"Status\": []}\n",
    "\n",
    "# Loop through each row and extract the required information\n",
    "for row in rows:\n",
    "    data[\"Issue ID\"].append(row.find(\"td\", class_=\"text--label table-grid--1\").text.strip().replace(\"\\n\", \" \"))\n",
    "    data[\"Product\"].append(row.find(\"td\", class_=\"text--label table-grid--2\").text.strip())\n",
    "    data[\"Description\"].append(row.find(\"td\", class_=\"field field--name-field-product-known-issue-text field--node field--label-hidden\").p.text.strip())\n",
    "    data[\"Status\"].append(row.find(\"td\", class_=\"text--label table-grid--2 relative\").text.strip())\n",
    "\n",
    "#for row in rows:\n",
    "#    issue_id.append(row.find(\"td\", class_=\"text--label table-grid--1\").text.strip().replace(\"\\n\", \" \"))\n",
    "#    product.append(row.find(\"td\", class_=\"text--label table-grid--2\").text.strip())\n",
    "#    desc_cell = row.find(\"td\", class_=\"field field--name-field-product-known-issue-text field--node field--label-hidden\")\n",
    "#    description.append(desc_cell.p.text.strip() if desc_cell is not None else \"\")\n",
    "#    status.append(row.find(\"td\", class_=\"text--label table-grid--2 relative\").text.strip())\n",
    "\n",
    "# Store the extracted information in a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Convert the dataframe to a CSV file\n",
    "filename = \"Tableau Known Issues - \" + str(datetime.datetime.now().date()) + \".csv\"\n",
    "df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dbfed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE                                index.html\r\n",
      "README.md                              main.ipynb\r\n",
      "Tableau Known Issues - 2023-02-08.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4378f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
